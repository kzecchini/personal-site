<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kevin Zecchini</title>
    <link>https://kzecchini.me/</link>
    <description>Recent content on Kevin Zecchini</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Mon, 21 Oct 2019 13:12:56 -0400</lastBuildDate><atom:link href="https://kzecchini.me/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About</title>
      <link>https://kzecchini.me/about/</link>
      <pubDate>Wed, 04 Oct 2023 11:16:00 -0400</pubDate>
      
      <guid>https://kzecchini.me/about/</guid>
      <description>Hi! I&amp;rsquo;m Kevin.
I started my data journey as a data scientist 8 years ago creating productized machine learning models in the healthcare and health adjacent spaces. These models span everything from back office churn/ltv models, to patient risk scores, to food recommendation systems. Many models I have implemented are still in use today!
In the process of implementing these models for use in customer facing applications, I&amp;rsquo;ve become an expert in MLOps and production engineering.</description>
    </item>
    
    <item>
      <title>We&#39;ll Do it Live!</title>
      <link>https://kzecchini.me/blog/do_it_live/</link>
      <pubDate>Mon, 21 Oct 2019 13:12:56 -0400</pubDate>
      
      <guid>https://kzecchini.me/blog/do_it_live/</guid>
      <description>We&amp;rsquo;ll Do it Live: Updating Machine Learning Models on Flask/uWSGI with No Downtime
At WW, I implemented a lightfm recommender model for their in-app social media platform. Every night the model was retrained for each global region, and the live service was updated without any downtime. The live model service was written in Flask and was deployed via k8s. I wrote a medium blog post about this for WW, which is linked at the top of the page.</description>
    </item>
    
  </channel>
</rss>
